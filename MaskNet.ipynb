{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3442ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8467401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('cloth_mask', 'mask_worn_incorrectly', 'n95_mask', 'no_mask', 'surgical_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dab2fc43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"image_folder = './data/images'\\nannotations_folder = './data/annotations'\\nxml_files = os.scandir(annotations_folder)\\nfor f in xml_files:\\n    xml = ET.parse(f)\\n    root = xml.getroot()\\n    image_file_name = root.find('filename').text\\n    #print(image_file_name)\\n    counter = 1\\n    for group in root.findall('object'):\\n        #print(group)\\n        bndbox = group.find('bndbox')\\n        xmin = int(bndbox.find('xmin').text)\\n        xmax = int(bndbox.find('xmax').text)\\n        ymin = int(bndbox.find('ymin').text)\\n        ymax = int(bndbox.find('ymax').text)\\n        \\n        im = Image.open(image_folder + '/' + image_file_name)\\n\\n \\n        # Cropped image of above dimension\\n        # (It will not change original image)\\n        im1 = im.crop((xmin,ymin,xmax,ymax))\\n \\n        #im1.show()\\n        im1.save('./data/augmented/' + str(counter) + '_' + image_file_name)\\n        counter = counter + 1\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''image_folder = './data/images'\n",
    "annotations_folder = './data/annotations'\n",
    "xml_files = os.scandir(annotations_folder)\n",
    "for f in xml_files:\n",
    "    xml = ET.parse(f)\n",
    "    root = xml.getroot()\n",
    "    image_file_name = root.find('filename').text\n",
    "    #print(image_file_name)\n",
    "    counter = 1\n",
    "    for group in root.findall('object'):\n",
    "        #print(group)\n",
    "        bndbox = group.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        \n",
    "        im = Image.open(image_folder + '/' + image_file_name)\n",
    "\n",
    " \n",
    "        # Cropped image of above dimension\n",
    "        # (It will not change original image)\n",
    "        im1 = im.crop((xmin,ymin,xmax,ymax))\n",
    " \n",
    "        #im1.show()\n",
    "        im1.save('./data/augmented/' + str(counter) + '_' + image_file_name)\n",
    "        counter = counter + 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de9ffc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath = './dataset/'\n",
    "transform = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])\n",
    "data = datasets.ImageFolder(root=imagePath, transform=transform)\n",
    "loader = DataLoader(dataset=data,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "797c5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image size normalization\n",
    "\n",
    "def get_mean_and_std(dataloader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        channels_sum += torch.mean(data, dim=[0,2,3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n",
    "        num_batches += 1\n",
    "    \n",
    "    mean = channels_sum / num_batches\n",
    "\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff02622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization:\n",
      "Mean: tensor([0.5295, 0.4899, 0.4659])\n",
      "Standard Dev: tensor([0.2976, 0.2912, 0.2968])\n"
     ]
    }
   ],
   "source": [
    "mean, std = get_mean_and_std(loader)\n",
    "print('Before normalization:')\n",
    "print('Mean: '+  str(mean))\n",
    "print('Standard Dev: '+  str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd065cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalization:\n",
      "Mean: tensor([-5.4230e-08, -1.0993e-07,  1.0962e-07])\n",
      "Standard Dev: tensor([1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=mean,std=std)])\n",
    "data = datasets.ImageFolder(root=imagePath, transform=transform)\n",
    "loader = DataLoader(data,batch_size=32)\n",
    "new_mean, new_std = get_mean_and_std(loader)\n",
    "print('After normalization:')\n",
    "print('Mean: '+  str(new_mean))\n",
    "print('Standard Dev: '+  str(new_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5eed220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1542\n",
      "Test size: 386\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "print('Train size: ' + str(len(train_dataset)))\n",
    "print('Test size: ' + str(len(test_dataset)))\n",
    "train_loader = DataLoader(train_dataset,batch_size=32)\n",
    "test_loader = DataLoader(test_dataset,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define MaskNet here\n",
    "class MaskNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convlayers =  nn.Sequential(\n",
    "            \n",
    "        )\n",
    "        self.FC = nn.Sequential(\n",
    "           \n",
    "            \n",
    "        )\n",
    "    \n",
    "    def forward(self,X):\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde26c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d8adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis and Run on test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e160ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './MaskNet.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
